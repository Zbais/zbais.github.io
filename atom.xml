<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-06T01:37:03.082Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大学物理 习题分析与解答（PDF）</title>
    <link href="http://example.com/2022/05/06/%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86-%E4%B9%A0%E9%A2%98%E5%88%86%E6%9E%90%E4%B8%8E%E8%A7%A3%E7%AD%94%EF%BC%88PDF%EF%BC%89/"/>
    <id>http://example.com/2022/05/06/%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86-%E4%B9%A0%E9%A2%98%E5%88%86%E6%9E%90%E4%B8%8E%E8%A7%A3%E7%AD%94%EF%BC%88PDF%EF%BC%89/</id>
    <published>2022-05-06T01:31:53.000Z</published>
    <updated>2022-05-06T01:37:03.082Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本答案虽为大学物理第二版习题解析与解答，但亲测第三版大学物理课后习题与第二版习题无异，故完全可以作为第二三版大学物理的课后习题解答</strong><br><strong>以下是我的百度网盘链接有需要的可自取</strong></p><p>点这     ——-&gt;   <a href="https://pan.baidu.com/s/1gEZkrZrVyHl3Ni6pezVA3Q">大学物理习题解析与解答</a><br>提取码：b98s</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;本答案虽为大学物理第二版习题解析与解答，但亲测第三版大学物理课后习题与第二版习题无异，故完全可以作为第二三版大学物理的课后习题解答&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;以下是我的百度网盘链接有需要的可自取&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;点这     —</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>要求按照考试成绩等级输出百分制分数段，A等为85分以上，B等为70-84分，C等为60~69分，D等为60分以下。成绩等级由键盘输入</title>
    <link href="http://example.com/2022/05/06/%E8%A6%81%E6%B1%82%E6%8C%89%E7%85%A7%E8%80%83%E8%AF%95%E6%88%90%E7%BB%A9%E7%AD%89%E7%BA%A7%E8%BE%93%E5%87%BA%E7%99%BE%E5%88%86%E5%88%B6%E5%88%86%E6%95%B0%E6%AE%B5%EF%BC%8CA%E7%AD%89%E4%B8%BA85%E5%88%86%E4%BB%A5%E4%B8%8A%EF%BC%8CB%E7%AD%89%E4%B8%BA70-84%E5%88%86%EF%BC%8CC%E7%AD%89%E4%B8%BA60-69%E5%88%86%EF%BC%8CD%E7%AD%89%E4%B8%BA60%E5%88%86%E4%BB%A5%E4%B8%8B%E3%80%82%E6%88%90%E7%BB%A9%E7%AD%89%E7%BA%A7%E7%94%B1%E9%94%AE%E7%9B%98%E8%BE%93%E5%85%A5/"/>
    <id>http://example.com/2022/05/06/%E8%A6%81%E6%B1%82%E6%8C%89%E7%85%A7%E8%80%83%E8%AF%95%E6%88%90%E7%BB%A9%E7%AD%89%E7%BA%A7%E8%BE%93%E5%87%BA%E7%99%BE%E5%88%86%E5%88%B6%E5%88%86%E6%95%B0%E6%AE%B5%EF%BC%8CA%E7%AD%89%E4%B8%BA85%E5%88%86%E4%BB%A5%E4%B8%8A%EF%BC%8CB%E7%AD%89%E4%B8%BA70-84%E5%88%86%EF%BC%8CC%E7%AD%89%E4%B8%BA60-69%E5%88%86%EF%BC%8CD%E7%AD%89%E4%B8%BA60%E5%88%86%E4%BB%A5%E4%B8%8B%E3%80%82%E6%88%90%E7%BB%A9%E7%AD%89%E7%BA%A7%E7%94%B1%E9%94%AE%E7%9B%98%E8%BE%93%E5%85%A5/</id>
    <published>2022-05-06T01:30:38.000Z</published>
    <updated>2022-05-06T01:38:32.620Z</updated>
    
    <content type="html"><![CDATA[<p><strong>输入你想知道的分数段，就可知道对应百分制分数</strong><br><strong>代码块如下</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;ctype.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="type">char</span> ch;</span><br><span class="line"><span class="type">int</span> flag = <span class="number">1</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;请输入您想查看的等级（请输入字母）：&quot;</span>);</span><br><span class="line"><span class="keyword">while</span>(flag)&#123;</span><br><span class="line">flag = <span class="number">0</span>;</span><br><span class="line">ch = getchar();</span><br><span class="line"><span class="keyword">switch</span>(<span class="built_in">toupper</span>(ch))&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;A&#x27;</span>: <span class="built_in">printf</span>(<span class="string">&quot;A等为85分以上&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;B&#x27;</span>: <span class="built_in">printf</span>(<span class="string">&quot;B等为70-84分&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;C&#x27;</span>: <span class="built_in">printf</span>(<span class="string">&quot;C等为60-69分&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&#x27;D&#x27;</span>: <span class="built_in">printf</span>(<span class="string">&quot;D等为60分以下&quot;</span>);<span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">default</span> : <span class="built_in">printf</span>(<span class="string">&quot;请输入正确格式（字母）:&quot;</span>);flag = <span class="number">1</span>;<span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125; </span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;输入你想知道的分数段，就可知道对应百分制分数&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;代码块如下&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;spa</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>工程数学线性代数第六版答案与解析,《线性代数附册学习辅导与习题全解》</title>
    <link href="http://example.com/2022/05/06/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%AC%E5%85%AD%E7%89%88%E7%AD%94%E6%A1%88%E4%B8%8E%E8%A7%A3%E6%9E%90-%E3%80%8A%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E9%99%84%E5%86%8C%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%AF%BC%E4%B8%8E%E4%B9%A0%E9%A2%98%E5%85%A8%E8%A7%A3%E3%80%8B/"/>
    <id>http://example.com/2022/05/06/%E5%B7%A5%E7%A8%8B%E6%95%B0%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%AC%E5%85%AD%E7%89%88%E7%AD%94%E6%A1%88%E4%B8%8E%E8%A7%A3%E6%9E%90-%E3%80%8A%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E9%99%84%E5%86%8C%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%AF%BC%E4%B8%8E%E4%B9%A0%E9%A2%98%E5%85%A8%E8%A7%A3%E3%80%8B/</id>
    <published>2022-05-06T01:29:14.000Z</published>
    <updated>2022-05-06T01:37:37.017Z</updated>
    
    <content type="html"><![CDATA[<p><strong>简介</strong><br>每到学期快结束，大家都会进入复习状态。就拿我来说我就是快到期末考试的前半个月复习，我会去看看以前做过的题，通常会带着答案一起看，网上答案一搜，搜到的答案让我心难受，要么是这给你一个答案步骤没解析；要么是题目对不上；更可气的是碰到一些”吊胃口的“，看了一小部分内容就要你去关注他公众号，或者收费。下面我推荐的一个链接可以跳转到由同济大学数学系编制的配套的《线性代数附册学习辅导与习题全解》的PDF文件，</p><p>请点击右侧链接  — &gt;<a href="https://nbccadminedupl-my.sharepoint.com/personal/personal_vldano_x2_tn/_layouts/15/onedrive.aspx?id=/personal/personal_vldano_x2_tn/Documents/%E5%90%8C%E6%B5%8E-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%88%E7%AC%AC%E5%85%AD%E7%89%88%EF%BC%89/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E9%99%84%E5%86%8C%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%AF%BC%E4%B8%8E%E4%B9%A0%E9%A2%98%E5%85%A8%E8%A7%A3.pdf&parent=/personal/personal_vldano_x2_tn/Documents/%E5%90%8C%E6%B5%8E-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%88%E7%AC%AC%E5%85%AD%E7%89%88%EF%BC%89&originalPath=aHR0cHM6Ly9uYmNjYWRtaW5lZHVwbC1teS5zaGFyZXBvaW50LmNvbS86YjovZy9wZXJzb25hbC9wZXJzb25hbF92bGRhbm9feDJfdG4vRWZjSWF5U0ZuTnhMaVZzb1lRSzdxQWdCTFRxNzVkV0xXWW5OaTRPQUY2aTNXdz9ydGltZT11ZXctSzFvTDJFZw">《线性代数附册学习辅导与习题全解》</a><br>或者点击此链接 —- &gt;<a href="https://nbccadminedupl-my.sharepoint.com/personal/personal_vldano_x2_tn/_layouts/15/onedrive.aspx?id=/personal/personal_vldano_x2_tn/Documents/%E5%90%8C%E6%B5%8E-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%88%E7%AC%AC%E5%85%AD%E7%89%88%EF%BC%89/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E9%99%84%E5%86%8C%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%AF%BC%E4%B8%8E%E4%B9%A0%E9%A2%98%E5%85%A8%E8%A7%A3.pdf&parent=/personal/personal_vldano_x2_tn/Documents/%E5%90%8C%E6%B5%8E-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%88%E7%AC%AC%E5%85%AD%E7%89%88%EF%BC%89&originalPath=aHR0cHM6Ly9uYmNjYWRtaW5lZHVwbC1teS5zaGFyZXBvaW50LmNvbS86YjovZy9wZXJzb25hbC9wZXJzb25hbF92bGRhbm9feDJfdG4vRWZjSWF5U0ZuTnhMaVZzb1lRSzdxQWdCTFRxNzVkV0xXWW5OaTRPQUY2aTNXdz9ydGltZT1VM3JVazU0TjJFZw">《线性代数附册学习辅导与习题全解》</a></p><p><strong>如果在线浏览失败，可下载我的百度网盘资源链接如下：<br><a href="https://pan.baidu.com/s/1a4Xo4tjCLhL-vaDVfzBFwA">线性代数附册学习辅导与习题全解—百度网盘资源下载</a><br>提取码：pkph</strong></p><p>希望能帮到苦苦寻找线性代数习题答案的你。<br><strong>如果这篇文章对你有帮助还望不要吝啬您的点赞收藏转发加关注</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;&lt;br&gt;每到学期快结束，大家都会进入复习状态。就拿我来说我就是快到期末考试的前半个月复习，我会去看看以前做过的题，通常会带着答案一起看，网上答案一搜，搜到的答案让我心难受，要么是这给你一个答案步骤没解析；要么是题目对不上；更可气的是碰到</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>通过python爬虫爬取图片，并通过企业微信机器人发送图片至企业微信群</title>
    <link href="http://example.com/2022/05/06/%E9%80%9A%E8%BF%87python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8F%91%E9%80%81%E5%9B%BE%E7%89%87%E8%87%B3%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E7%BE%A4/"/>
    <id>http://example.com/2022/05/06/%E9%80%9A%E8%BF%87python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8F%91%E9%80%81%E5%9B%BE%E7%89%87%E8%87%B3%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E7%BE%A4/</id>
    <published>2022-05-06T01:27:41.000Z</published>
    <updated>2022-05-06T01:39:52.582Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习了一点爬虫，就想着能不能通过企业微信的机器人发送一些图片到微信群中。企业微信机器人说到底就是一个webhook地址，你也可以看成一个URL,而利用企业微信机器人发送图片实则就是向服务器发送post请求，再将响应数据发送到webhook地址，以下是我爬取   <a href="http://pic.netbian.com/">彼岸图网</a>（<a href="http://pic.netbian.com)的python爬虫代码,使用的是强大的requests库(scrapy框架还不太会用,就不用了)/">http://pic.netbian.com）的python爬虫代码，使用的是强大的requests库（scrapy框架还不太会用,就不用了）</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> os,re,time,hashlib,base64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">head = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">start_urls = []     <span class="comment">#定义一个url列表</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">1000</span>):</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">1</span>):       </span><br><span class="line">        url = <span class="string">&#x27;http://pic.netbian.com/index.html&#x27;</span>   <span class="comment">#判断是否为首页，并加入url列表中</span></span><br><span class="line">        start_urls.append(url)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        url = <span class="string">&#x27;http://pic.netbian.com/index_&#x27;</span>+<span class="built_in">str</span>(i)+<span class="string">&#x27;.html&#x27;</span>    <span class="comment">#判断是否为非首页，并加入url列表中</span></span><br><span class="line">        start_urls.append(url)</span><br><span class="line">i = <span class="number">1</span>   <span class="comment">#为了将图片每二十张放入到不同的文件夹</span></span><br><span class="line">url1=<span class="string">&#x27;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx&#x27;</span> <span class="comment">#自己的webhoook地址</span></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> start_urls:              <span class="comment">#将url列表url依次取出</span></span><br><span class="line">    response = requests.get(url)        <span class="comment">#向服务器发出get请求，获得响应</span></span><br><span class="line">    response.encoding=<span class="string">&#x27;gbk&#x27;</span>                 <span class="comment">#设置响应的编码格式为GBK</span></span><br><span class="line">    html = response.text                        <span class="comment">#获得相应的文本信息</span></span><br><span class="line">    soup = BeautifulSoup(html,<span class="string">&#x27;lxml&#x27;</span>)               <span class="comment">#通过beautifulsoup库中的lxml HTML 解析器提取数据</span></span><br><span class="line">    result = soup.find(<span class="string">&#x27;ul&#x27;</span>,class_=<span class="string">&quot;clearfix&quot;</span>).find_all(<span class="string">&#x27;img&#x27;</span>)      <span class="comment">#找到&lt;img&gt;标签</span></span><br><span class="line">    </span><br><span class="line">    root = <span class="string">&#x27;D:/page/SpiderImgs/imgs-&#x27;</span>+<span class="built_in">str</span>((i//<span class="number">20</span>)+<span class="number">1</span>)+<span class="string">&#x27;/&#x27;</span>            <span class="comment">#拼接保存的路径  根据自己要保存的路径替换</span></span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> result:</span><br><span class="line">        path = root + img[<span class="string">&#x27;src&#x27;</span>].split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>] <span class="comment">#获取图片资源名称(http://pic.netbian.com/uploads/allimg/190824/212516-1566653116f355.jpg中的212516-1566653116f355.jpg)并拼接保存路径</span></span><br><span class="line">        url = <span class="string">&#x27;http://pic.netbian.com&#x27;</span>+img[<span class="string">&#x27;src&#x27;</span>]   <span class="comment">#拼接图片的url</span></span><br><span class="line">        r = requests.get(url, headers=head)         <span class="comment">#发送get请求图片资源</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(root):                <span class="comment">#判断文件夹是否存在</span></span><br><span class="line">            os.makedirs(root)                       <span class="comment">#创建新的文件夹</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span> (path,<span class="string">&#x27;wb+&#x27;</span>) <span class="keyword">as</span> f:            <span class="comment">#创建一个文件以二进制写</span></span><br><span class="line">                f.write(r.content)                  <span class="comment">#将请求服务器图片资源的响应结果写进文件中</span></span><br><span class="line">                f.close()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;图片 &quot;</span>+<span class="built_in">str</span>(i)+<span class="string">&quot;.jpg 保存成功&quot;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:              <span class="comment">#以读二进制的方式打开文件</span></span><br><span class="line">                base64_data = base64.b64encode(f.read()).decode() <span class="comment">#获取base64编码</span></span><br><span class="line">            file = <span class="built_in">open</span>(path,<span class="string">&#x27;rb&#x27;</span>)              </span><br><span class="line">            md = hashlib.md5()</span><br><span class="line">            md.update(file.read())</span><br><span class="line">            res1 = md.hexdigest()       <span class="comment">#获取图片内容的md5值</span></span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">&quot;msgtype&quot;</span> : <span class="string">&#x27;image&#x27;</span>,</span><br><span class="line">                <span class="string">&quot;image&quot;</span>:&#123;</span><br><span class="line">                    <span class="string">&quot;base64&quot;</span>:base64_data,</span><br><span class="line">                    <span class="string">&quot;md5&quot;</span>:res1</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;               <span class="comment">#JSON数据格式</span></span><br><span class="line">            t = requests.post(url1, headers=head, json=data) <span class="comment">#发送post请求</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;异常&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        i = i + <span class="number">1</span>                       <span class="comment">#文件夹名称加一</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)                       <span class="comment">#让程序睡3秒</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上代码有任何问题或不懂可在评论区留言，</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近学习了一点爬虫，就想着能不能通过企业微信的机器人发送一些图片到微信群中。企业微信机器人说到底就是一个webhook地址，你也可以看成一个URL,而利用企业微信机器人发送图片实则就是向服务器发送post请求，再将响应数据发送到webhook地址，以下是我爬取   &lt;a h</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>idea java:程序包×××.××××.××不存在。解决方法，</title>
    <link href="http://example.com/2022/05/06/idea-java-%E7%A8%8B%E5%BA%8F%E5%8C%85%C3%97%C3%97%C3%97-%C3%97%C3%97%C3%97%C3%97-%C3%97%C3%97%E4%B8%8D%E5%AD%98%E5%9C%A8%E3%80%82%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%8C/"/>
    <id>http://example.com/2022/05/06/idea-java-%E7%A8%8B%E5%BA%8F%E5%8C%85%C3%97%C3%97%C3%97-%C3%97%C3%97%C3%97%C3%97-%C3%97%C3%97%E4%B8%8D%E5%AD%98%E5%9C%A8%E3%80%82%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%8C/</id>
    <published>2022-05-06T01:26:32.000Z</published>
    <updated>2022-05-06T01:38:11.547Z</updated>
    
    <content type="html"><![CDATA[<p><strong>先粘一张报错截图，不一样的可以寻找下一个解决方法</strong></p><p><img src="https://img-blog.csdnimg.cn/20210202230322244.png#pic_center" alt="在这里插入图片描述"><br>我在网上找了几种方法都无法解决我的问题，然后自己尝试解决了，先说我的解决方法吧</p><p>1）.某某包不存在，我想到最笨的方法就是，不存在，不就换一个就好</p><p><strong>这是我报错的包</strong><br><img src="https://img-blog.csdnimg.cn/20210202231520622.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hZTDM0MjMwMA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>换成这个</strong><br><img src="https://img-blog.csdnimg.cn/20210202234746195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hZTDM0MjMwMA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>idea有强大的代码提示功能，所以大可一个一个试的去<br><img src="https://img-blog.csdnimg.cn/20210202235115743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hZTDM0MjMwMA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>最后成功运行<br><img src="https://img-blog.csdnimg.cn/20210203000301868.png" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;先粘一张报错截图，不一样的可以寻找下一个解决方法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20210202230322244.png#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>python爬虫使用selenium动态加载（下拉加载）爬取取环球网疫情新闻标题和链接，</title>
    <link href="http://example.com/2022/05/06/python%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8selenium%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%EF%BC%88%E4%B8%8B%E6%8B%89%E5%8A%A0%E8%BD%BD%EF%BC%89%E7%88%AC%E5%8F%96%E5%8F%96%E7%8E%AF%E7%90%83%E7%BD%91%E7%96%AB%E6%83%85%E6%96%B0%E9%97%BB%E6%A0%87%E9%A2%98%E5%92%8C%E9%93%BE%E6%8E%A5%EF%BC%8C/"/>
    <id>http://example.com/2022/05/06/python%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8selenium%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%EF%BC%88%E4%B8%8B%E6%8B%89%E5%8A%A0%E8%BD%BD%EF%BC%89%E7%88%AC%E5%8F%96%E5%8F%96%E7%8E%AF%E7%90%83%E7%BD%91%E7%96%AB%E6%83%85%E6%96%B0%E9%97%BB%E6%A0%87%E9%A2%98%E5%92%8C%E9%93%BE%E6%8E%A5%EF%BC%8C/</id>
    <published>2022-05-06T01:25:14.000Z</published>
    <updated>2022-05-06T01:44:43.477Z</updated>
    
    <content type="html"><![CDATA[<p>当你爬数据的时候有没有遇到过向某个URL请求数据，响应回来的页面源码不全，明明在浏览器打开能看到，可到自己爬的时候就是看不到。其实是因为你爬取的页面是动态网页，很多数据是要加载才能渲染出来的。比如爬取环球网文章页面： <a href="https://world.huanqiu.com/article">环球网</a>.</p><p><img src="https://img-blog.csdnimg.cn/20210331130357644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hZTDM0MjMwMA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这个时候通过request库就不太适合爬取动态网页了。目前主流是通过selenium去爬取。</p><p><strong>Selenium介绍：</strong></p><ol><li>Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接调用浏览器，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏等。</li></ol><p>在使用selenium爬取数据之前必然要先下载selenium，找到你python下的Scripts文件夹，复制路径，用cmd打开，输入pip install selenium<br><img src="https://img-blog.csdnimg.cn/20210331131339409.png" alt="在这里插入图片描述"><br>然后就去下载浏览器驱动<br>首先查看浏览器版本，本文介绍Chrome，其他浏览器自行百度。 <a href="chrome://version/">点击查看Chrome版本</a><br> chromedriver与chrome各版本及下载地址： <a href="https://blog.csdn.net/cz9025/article/details/70160273?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&dist_request_id=1328761.728.16171688994638669&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control">具体看这位大佬的博客</a></p><hr><p><strong>接下来就展示一下如何使用selenium动态加载（下拉加载）获取环球网疫情新闻标题和链接，</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span>  requests, time, re                        <span class="comment">#</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup           <span class="comment">#使用bs4解析</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver          <span class="comment">#自动化测试工具</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    url = <span class="string">&quot;https://world.huanqiu.com/article&quot;</span>       <span class="comment">#环球网文章url</span></span><br><span class="line">    headers = &#123;<span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36&quot;</span>&#125;</span><br><span class="line">    save_path = <span class="string">&quot;./news.txt&quot;</span>                        <span class="comment">#疫情文章和链接保存路径</span></span><br><span class="line">    get_data(url)                                   <span class="comment">#获取网页数据</span></span><br><span class="line">    parse_data(url)                                 <span class="comment">#解析提取所需数据</span></span><br><span class="line">    save_data(save_path,url)                        <span class="comment">#保存数据</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">url</span>):</span><br><span class="line">    browser = webdriver.Chrome()                     <span class="comment">#创建Chrome浏览器对象，这会在电脑上在打开一个浏览器窗口</span></span><br><span class="line">    browser.get(url)                                 <span class="comment">#通过浏览器向服务器发送URL请求</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):                               </span><br><span class="line">        browser.execute_script(<span class="string">&quot;window.scrollTo(0,document.body.scrollHeight)&quot;</span>)         <span class="comment"># 将滚动条调整至页面底部循环三次</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">    response = browser.page_source                      <span class="comment"># 获取页面源码</span></span><br><span class="line">    soup = BeautifulSoup(response, <span class="string">&#x27;lxml&#x27;</span>)              <span class="comment"># beautifulsoup 使用  lxml 解析</span></span><br><span class="line">    <span class="keyword">return</span> soup</span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_data</span>(<span class="params">url</span>):</span><br><span class="line">    titles = []                                     <span class="comment">#爬取到的标题列表</span></span><br><span class="line">    hrefs = []                                      <span class="comment">#爬取到的标题链接列表</span></span><br><span class="line">    target_titles = []                              <span class="comment">#爬取到的疫情新闻标题列表</span></span><br><span class="line">    target_hrefs = []                               <span class="comment">#爬取到的疫情新闻标题链接列表</span></span><br><span class="line">    soup = get_data(url)</span><br><span class="line">    title_list = soup.select(<span class="string">&quot;.m-recommend-con ul li a div h4&quot;</span>)     <span class="comment">#调用soup的选择器方法找到标题位置，返回值为列表</span></span><br><span class="line">    href_list = soup.select(<span class="string">&quot;.m-recommend-con ul li a&quot;</span>)             <span class="comment">#调用soup的选择器方法找到标题链接位置，返回值为列表</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> title_list:</span><br><span class="line">        title = t.string</span><br><span class="line">        titles.append(title)                        <span class="comment">#添加至标题列表</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> href_list:</span><br><span class="line">        href = h[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">        hrefs.append(href)                          <span class="comment">#添加至标题链接列表</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> title <span class="keyword">in</span> titles:</span><br><span class="line">        <span class="keyword">if</span>(((<span class="string">&#x27;新冠&#x27;</span> <span class="keyword">in</span> title)|(<span class="string">&#x27;疫苗&#x27;</span> <span class="keyword">in</span> title)|(<span class="string">&#x27;疫情&#x27;</span> <span class="keyword">in</span> title)|(<span class="string">&#x27;病毒&#x27;</span> <span class="keyword">in</span> title))):          <span class="comment">#通过关键字来获取疫情相关信息</span></span><br><span class="line">            target_titles.append(titles[i])                                                     <span class="comment">#添加至疫情新闻标题列表</span></span><br><span class="line">            target_hrefs.append(<span class="string">&quot;https://world.huanqiu.com&quot;</span>+hrefs[i])                           <span class="comment">#添加至疫情新闻标题链接列表，并通过字符串拼接成url</span></span><br><span class="line">        i = i+<span class="number">1</span>                             </span><br><span class="line">    <span class="keyword">return</span> target_titles,target_hrefs</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_data</span>(<span class="params">save_path,url</span>):</span><br><span class="line">    target_titles,target_hrefs = parse_data(url)</span><br><span class="line">    f = <span class="built_in">open</span>(save_path, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)                                              <span class="comment">#以追加的方式打开文件，如果不存在则创建，设置字符集为utf-8</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(target_titles)):                                             </span><br><span class="line">        f.write(target_titles[i])                                                           <span class="comment">#写入疫情新闻标题列表</span></span><br><span class="line">        f.write(<span class="string">&quot;\n&quot;</span>)                                                                       <span class="comment">#换行</span></span><br><span class="line">        f.write(target_hrefs[i])                                                            <span class="comment">#写入疫情新闻标题链接列表</span></span><br><span class="line">        f.write(<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line">    f.close()                                                                               <span class="comment">#关闭</span></span><br><span class="line">    <span class="keyword">pass</span>                                                                    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;当你爬数据的时候有没有遇到过向某个URL请求数据，响应回来的页面源码不全，明明在浏览器打开能看到，可到自己爬的时候就是看不到。其实是因为你爬取的页面是动态网页，很多数据是要加载才能渲染出来的。比如爬取环球网文章页面： &lt;a href=&quot;https://world.huanq</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/05/06/elasticsearch%E5%AE%89%E8%A3%85elasticsearch-analysis-ik%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%8F%92%E4%BB%B6%EF%BC%8C%E7%BD%91%E9%80%9F%E5%A4%AA%E6%85%A2%EF%BC%8C%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98%E3%80%81%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF/"/>
    <id>http://example.com/2022/05/06/elasticsearch%E5%AE%89%E8%A3%85elasticsearch-analysis-ik%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%8F%92%E4%BB%B6%EF%BC%8C%E7%BD%91%E9%80%9F%E5%A4%AA%E6%85%A2%EF%BC%8C%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98%E3%80%81%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF/</id>
    <published>2022-05-06T01:16:25.320Z</published>
    <updated>2022-05-06T01:23:18.419Z</updated>
    
    <content type="html"><![CDATA[<p>elasticsearch安装elasticsearch-analysis-ik中文分词插件，网速太慢，版本问题、安装失败解决思路</p><p>[TOC]</p><h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>安装完Elasticsearch6.2.2后想再安装一个中文分词插件elasticsearch-analysis-ik，遇到了一下几个问题：</p><ol><li><p>本地JDK与elasticsearch-analysis-ik的版本不一致问题。</p></li><li><p>elasticsearch-analysis-ik-6.2.2下载速度过慢导致安装失败。<br><img src="https://img-blog.csdnimg.cn/c4390ddf4451434a80c28a0a172d1b70.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZKI5p-P5qCR,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li></ol><h1 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h1><ol><li>问题一：切换版本<br>笔者本地安装的是jdk1.8所以选择elasticsearch-6.2.2 和elasticsearch-analysis-ik-6.2.2<img src="https://img-blog.csdnimg.cn/00bc937573644744b054ec8bb3db5379.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZKI5p-P5qCR,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/388339779f1d4cfd86445322e65ec646.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZKI5p-P5qCR,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>问题二：<br>可以使用GitHub加速器将elasticsearch-analysis-ik-6.2.2.zip下载到本地安装。命令：<code>elasticsearch-plugin install file:///D:/Program/Elasticsearch8.0.0/elasticsearch-analysis-ik-6.2.2.zip</code><br>注意点：路径前面要加协议：file:&#x2F;&#x2F;&#x2F;。不然出现未知协议错误。<br><img src="https://img-blog.csdnimg.cn/fe7d6aea8f054980b788ec07a5da8cc9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6ZKI5p-P5qCR,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ol><h1 id="结尾（资源网盘下载）"><a href="#结尾（资源网盘下载）" class="headerlink" title="结尾（资源网盘下载）"></a>结尾（资源网盘下载）</h1><p>网盘：<a href="https://pan.baidu.com/s/1GPty6LC6jJCMZ3_tImiWKg?pwd=1024">elasticsearch-6.2.2</a> </p><p><strong>如果本文有帮助到你，欢迎点赞收藏加关注</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;elasticsearch安装elasticsearch-analysis-ik中文分词插件，网速太慢，版本问题、安装失败解决思路&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;问题背景&quot;&gt;&lt;a href=&quot;#问题背景&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2022/05/05/hello-world/"/>
    <id>http://example.com/2022/05/05/hello-world/</id>
    <published>2022-05-05T15:57:40.177Z</published>
    <updated>2022-05-06T01:24:28.581Z</updated>
    
    <content type="html"><![CDATA[<p>当你爬数据的时候有没有遇到过向某个URL请求数据，响应回来的页面源码不全，明明在浏览器打开能看到，可到自己爬的时候就是看不到。其实是因为你爬取的页面是动态网页，很多数据是要加载才能渲染出来的。比如爬取环球网文章页面： <a href="https://world.huanqiu.com/article">环球网</a>.</p><p><img src="https://img-blog.csdnimg.cn/20210331130357644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1hZTDM0MjMwMA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这个时候通过request库就不太适合爬取动态网页了。目前主流是通过selenium去爬取。</p><p><strong>Selenium介绍：</strong></p><ol><li>Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接调用浏览器，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏等。</li></ol><p>在使用selenium爬取数据之前必然要先下载selenium，找到你python下的Scripts文件夹，复制路径，用cmd打开，输入pip install selenium<br><img src="https://img-blog.csdnimg.cn/20210331131339409.png" alt="在这里插入图片描述"><br>然后就去下载浏览器驱动<br>首先查看浏览器版本，本文介绍Chrome，其他浏览器自行百度。 <a href="chrome://version/">点击查看Chrome版本</a><br> chromedriver与chrome各版本及下载地址： <a href="https://blog.csdn.net/cz9025/article/details/70160273?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&dist_request_id=1328761.728.16171688994638669&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control">具体看这位大佬的博客</a></p><hr><p><strong>接下来就展示一下如何使用selenium动态加载（下拉加载）获取环球网疫情新闻标题和链接，</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span>  requests, time, re                        <span class="comment">#</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup           <span class="comment">#使用bs4解析</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver          <span class="comment">#自动化测试工具</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    url = <span class="string">&quot;https://world.huanqiu.com/article&quot;</span>       <span class="comment">#环球网文章url</span></span><br><span class="line">    headers = &#123;<span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36&quot;</span>&#125;</span><br><span class="line">    save_path = <span class="string">&quot;./news.txt&quot;</span>                        <span class="comment">#疫情文章和链接保存路径</span></span><br><span class="line">    get_data(url)                                   <span class="comment">#获取网页数据</span></span><br><span class="line">    parse_data(url)                                 <span class="comment">#解析提取所需数据</span></span><br><span class="line">    save_data(save_path,url)                        <span class="comment">#保存数据</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">url</span>):</span><br><span class="line">    browser = webdriver.Chrome()                     <span class="comment">#创建Chrome浏览器对象，这会在电脑上在打开一个浏览器窗口</span></span><br><span class="line">    browser.get(url)                                 <span class="comment">#通过浏览器向服务器发送URL请求</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):                               </span><br><span class="line">        browser.execute_script(<span class="string">&quot;window.scrollTo(0,document.body.scrollHeight)&quot;</span>)         <span class="comment"># 将滚动条调整至页面底部循环三次</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">    response = browser.page_source                      <span class="comment"># 获取页面源码</span></span><br><span class="line">    soup = BeautifulSoup(response, <span class="string">&#x27;lxml&#x27;</span>)              <span class="comment"># beautifulsoup 使用  lxml 解析</span></span><br><span class="line">    <span class="keyword">return</span> soup</span><br><span class="line"></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_data</span>(<span class="params">url</span>):</span><br><span class="line">    titles = []                                     <span class="comment">#爬取到的标题列表</span></span><br><span class="line">    hrefs = []                                      <span class="comment">#爬取到的标题链接列表</span></span><br><span class="line">    target_titles = []                              <span class="comment">#爬取到的疫情新闻标题列表</span></span><br><span class="line">    target_hrefs = []                               <span class="comment">#爬取到的疫情新闻标题链接列表</span></span><br><span class="line">    soup = get_data(url)</span><br><span class="line">    title_list = soup.select(<span class="string">&quot;.m-recommend-con ul li a div h4&quot;</span>)     <span class="comment">#调用soup的选择器方法找到标题位置，返回值为列表</span></span><br><span class="line">    href_list = soup.select(<span class="string">&quot;.m-recommend-con ul li a&quot;</span>)             <span class="comment">#调用soup的选择器方法找到标题链接位置，返回值为列表</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> title_list:</span><br><span class="line">        title = t.string</span><br><span class="line">        titles.append(title)                        <span class="comment">#添加至标题列表</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> href_list:</span><br><span class="line">        href = h[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">        hrefs.append(href)                          <span class="comment">#添加至标题链接列表</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> title <span class="keyword">in</span> titles:</span><br><span class="line">        <span class="keyword">if</span>(((<span class="string">&#x27;新冠&#x27;</span> <span class="keyword">in</span> title)|(<span class="string">&#x27;疫苗&#x27;</span> <span class="keyword">in</span> title)|(<span class="string">&#x27;疫情&#x27;</span> <span class="keyword">in</span> title)|(<span class="string">&#x27;病毒&#x27;</span> <span class="keyword">in</span> title))):          <span class="comment">#通过关键字来获取疫情相关信息</span></span><br><span class="line">            target_titles.append(titles[i])                                                     <span class="comment">#添加至疫情新闻标题列表</span></span><br><span class="line">            target_hrefs.append(<span class="string">&quot;https://world.huanqiu.com&quot;</span>+hrefs[i])                           <span class="comment">#添加至疫情新闻标题链接列表，并通过字符串拼接成url</span></span><br><span class="line">        i = i+<span class="number">1</span>                             </span><br><span class="line">    <span class="keyword">return</span> target_titles,target_hrefs</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_data</span>(<span class="params">save_path,url</span>):</span><br><span class="line">    target_titles,target_hrefs = parse_data(url)</span><br><span class="line">    f = <span class="built_in">open</span>(save_path, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)                                              <span class="comment">#以追加的方式打开文件，如果不存在则创建，设置字符集为utf-8</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(target_titles)):                                             </span><br><span class="line">        f.write(target_titles[i])                                                           <span class="comment">#写入疫情新闻标题列表</span></span><br><span class="line">        f.write(<span class="string">&quot;\n&quot;</span>)                                                                       <span class="comment">#换行</span></span><br><span class="line">        f.write(target_hrefs[i])                                                            <span class="comment">#写入疫情新闻标题链接列表</span></span><br><span class="line">        f.write(<span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line">    f.close()                                                                               <span class="comment">#关闭</span></span><br><span class="line">    <span class="keyword">pass</span>                                                                    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;当你爬数据的时候有没有遇到过向某个URL请求数据，响应回来的页面源码不全，明明在浏览器打开能看到，可到自己爬的时候就是看不到。其实是因为你爬取的页面是动态网页，很多数据是要加载才能渲染出来的。比如爬取环球网文章页面： &lt;a href=&quot;https://world.huanq</summary>
      
    
    
    
    
  </entry>
  
</feed>
